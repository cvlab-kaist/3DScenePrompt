<html>

<head>
    <meta charset="utf-8" />
    <title>3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</title>

    <!-- Favicon references -->
    <link rel="icon" type="image/png" href="./images/logo.png">
    <link rel="apple-touch-icon" href="./images/logo.png">
    <link rel="icon" type="image/x-icon" href="favicon.ico">

    <meta
        content="3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation"
        name="description" />
    <meta
        content="3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation"
        property="og:title" />
    <meta
        content="3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation"
        property="og:description" />
    <meta
        content="3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation"
        property="twitter:title" />
    <meta
        content="3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation"
        property="twitter:description" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"
        crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&family=Varela+Round&display=swap"
        rel="stylesheet">
    <link href="style.css?" rel="stylesheet" type="text/css" />

    <!-- 🔎 Added minimal CSS for click-to-zoom lightbox -->
    <style>
      img.zoomable { cursor: zoom-in; transition: transform .2s ease; }
      .lightbox-overlay {
        position: fixed; inset: 0; display: none; align-items: center; justify-content: center;
        background: rgba(0,0,0,.9); z-index: 10000;
      }
      .lightbox-overlay.active { display: flex; }
      .lightbox-overlay img { max-width: 95vw; max-height: 95vh; box-shadow: 0 10px 40px rgba(0,0,0,.6); border-radius: 8px; }
      .lightbox-overlay, .lightbox-overlay * { cursor: zoom-out; }
      body.no-scroll { overflow: hidden; }

      /* 🧭 Title alignment fix */
      .title-row {
        position: relative;
        width: 100%;
        text-align: center;
        margin-top: 20px;
      }
      .title-center {
        display: inline-flex;
        align-items: center;
        gap: 10px;
        position: relative;
        left: 50%;
        transform: translateX(-50%);
      }
      .title-video {
        width: 120px;
        height: auto;
        border-radius: 20px;
      }
      .title-text-block {
        text-align: center;
      }
    </style>

    <!-- MathJax for LaTeX rendering -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']]
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
</head>

<body>
    <header class="site-header">
        <div class="container">
            <nav class="main-nav">
                <ul class="nav-links">
                    <li><a href="#teaser">Teaser</a></li>
                    <li><a href="#overview">Overview</a></li>
                    <li><a href="#analysis">Analysis</a></li>
                    <li><a href="#results">Results</a></li>
                    <li><a href="#citation">Citation</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="hero-section">
        <div class="container">
            <div class="title-row" 
                style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
            <div class="title-flex" 
                style="display: flex; align-items: center; justify-content: center; gap: 12px; text-align: center; margin: 0 auto;">

                <!-- 🎥 로고 -->
                <video class="title-video" 
                    autoplay 
                    muted 
                    loop 
                    playsinline 
                    style="width: 130px; height: auto; border-radius: 20px; transform: scaleX(1);">
                    <source src="images/logo_moving.mp4" type="video/mp4">
                </video>

                <!-- 🧠 타이틀 텍스트 -->
                <div class="title-text-block" style="text-align: center; max-width: 700px;">
                    <h1 class="title" style="font-size: 1.8em; font-weight: 700; margin: 0;">
                    3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation
                    </h1>
                    <h1 class="subtitle" style="font-size: 1.1em; color: #333; margin-top: 5px;">
                    arXiv 2025
                    </h1>
                </div>
            </div>
            </div>

            <!-- 이하 기존 내용 유지 -->
            <div class="base-row author-row">
                <div class="base-col author-col">
                    <a href="https://scholar.google.com/citations?user=0H3dcPoAAAAJ&hl=en" target="_blank" class="author-text">
                        Joungbin Lee<sup>1</sup><span class="equal-author">*</span>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://crepejung00.github.io" target="_blank" class="author-text">
                        Jaewoo Jung<sup>1</sup><span class="equal-author">*</span>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://onground-korea.github.io" target="_blank" class="author-text">
                        Jisang Han<sup>1</sup><span class="equal-author">*</span>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://scholar.google.com/citations?user=D3h3NxwAAAAJ&hl=en" target="_blank" class="author-text">
                        Takuya Narihira<sup>2</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://ai.sony/people/Kazumi-Fukuda/" target="_blank" class="author-text">
                        Kazumi Fukuda<sup>2</sup>
                    </a>
                </div>
            </div>

            <div class="base-row author-row">
                <div class="base-col author-col">
                    <a href="https://j0seo.github.io" target="_blank" class="author-text">
                        Junyoung Seo<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://sunghwanhong.github.io" target="_blank" class="author-text">
                        Sunghwan Hong<sup>3</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://www.yukimitsufuji.com" target="_blank" class="author-text">
                        Yuki Mitsufuji<sup>2,4</sup><span class="corresponding-author">†</span>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://cvlab.kaist.ac.kr/members/faculty" target="_blank" class="author-text">
                        Seungryong Kim<sup>1</sup><span class="corresponding-author">†</span>
                    </a>
                </div>
            </div>

            <div class="base-row author-row">
                <div class="base-col author-col affiliations">
                    <sup>1</sup>KAIST AI &nbsp;&nbsp;
                    <sup>2</sup>Sony AI &nbsp;&nbsp;
                    <sup>3</sup>ETH Zürich &nbsp;&nbsp;
                    <sup>4</sup>Sony Group Corporation
                    <br><br>
                    <span class="equal-author-note">*</span> Equal contribution &nbsp;&nbsp;
                    <span class="corresponding-author-note">†</span> Co-corresponding authors
                </div>
            </div>

            <div class="link-labels base-row"> 
                <div class="base-col icon-col">
                    <a href="" target="_blank" class="link-block">
                        <i class="fa fa-university main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">arXiv</strong>
                    </a>
                </div>

                <div class="base-col icon-col">
                    <a href="" target="_blank" class="link-block">
                        <i class="fa fas fa-file-text main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Paper</strong>
                    </a>
                </div>

                <div class="base-col icon-col">
                    <a href="https://github.com/cvlab-kaist/3DScenePrompt" target="_blank" class="link-block">
                        <i class="fa fa-github main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Code</strong>
                    </a>
                </div>

                <div class="base-col icon-col">
                    <a href="#citation" class="link-block">
                        <i class="fa fa-graduation-cap main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Citation</strong>
                    </a>
                </div>
            </div>    
        </div>
    </div>
</body>
</html>

    <main class="main-content">
        <div class="container">
            
            <!-- Contents: TL;DR -->
            
            <section id="teaser" class="section">
                <div class="container">
                    <div class="tldr">
                    <b>TL;DR</b>: <strong>3DScenePrompt</strong> is a framework to generate a <strong>next chunk video</strong> from <strong>any arbitrary-length</strong> in-the-wild input video while allowing precise <strong>camera control</strong> and maintaining <strong>scene-consistency</strong> with the input video.
                    </div>

                    <h2>Teaser</h2>

                    <!-- 🎥 Teaser 영상 섹션 (Results와 동일한 구조) -->
                    <div class="slideshow-container">
                    <div class="slideshow" data-range="001-009" data-dir="videos" data-ext="mp4"></div>
                    <button class="slideshow-nav prev" aria-label="Previous slide" title="Previous">&#10094;</button>
                    <button class="slideshow-nav next" aria-label="Next slide" title="Next">&#10095;</button>
                    <div class="slideshow-dots"></div>
                    </div>
                </div>
            </section>



            <section id="overview" class="section">
            <h2>Overview</h2>
            <div class="image-container">
                <div class="image-content">
                <img src="images/fig_framework.png" class="img large-image" alt="Framework overview">
                </div>
                <p class="image-caption">
                <strong>Overview of our proposed framework.</strong>
                Given a static 3D point cloud reconstructed from an input video and an action prompt, our model generates a dynamic video aligned with a user-specified camera trajectory. 
                The framework jointly leverages 
                <span style="color:#0070C0;"><strong>temporal</strong></span> conditioning from the last few frames to ensure motion continuity, and 
                <span style="color:#06B050;"><strong>spatial</strong></span> conditioning from rendered views of the static point cloud to preserve scene geometry.
                </p>
            </div>

            <p>
                We present <strong>3DScenePrompt</strong>, a framework for <strong>scene-consistent, camera-controllable video generation</strong> that extends arbitrary-length input videos along user-specified camera trajectories while preserving scene geometry.
                The key idea is a <strong>dual spatio-temporal conditioning</strong> strategy that conditions on both 
                <span style="color:#0070C0;"><strong>temporal</strong></span> cues (last few frames for motion continuity) and 
                <span style="color:#06B050;"><strong>spatial</strong></span> cues (geometry-aware views for scene consistency).
            </p>

            <p>
                Directly reusing spatially adjacent frames can leak past dynamics into the future. To avoid this, we build a <strong>3D scene memory</strong> that stores only <strong>static geometry</strong>, reconstructed via dynamic SLAM and refined with a <strong>dynamic masking</strong> pipeline that removes moving objects.
            </p>

            <p>
                From this memory, we render <strong>projected static views</strong> that act as <strong>3D scene prompts</strong>, giving geometrically accurate spatial guidance while temporal conditioning drives natural motion. 
                This enables precise camera control, long-range spatial coherence, and efficient computation.
            </p>
            </section>
          


            
            <section id="analysis" class="section">
            <h2>Analysis</h2>

            <!-- 🔹 Part 1: Importance -->
            <h3>Importance of Eliminating Dynamic Regions</h3>
            <div class="image-container">
                <div class="image-content">
                <img src="images/fig_analysis.png" class="img large-image" alt="Dynamic Masking illustration">
                </div>
                <p class="image-caption">
                <strong>Illustration of dynamic masking for static scene extraction.</strong><br>
                (a) Without masking, moving objects create ghosting artifacts across frames.<br>
                (b) With our dynamic masking pipeline, dynamic elements are identified and removed,
                yielding clean static-only point clouds.
                </p>
            </div>

            <p>
                <strong>Dynamic masking</strong> plays a crucial role in separating motion from structure
                in scene-consistent video generation. By removing transient moving objects before reconstructing
                the 3D scene, it prevents ghosting artifacts and ensures that only
                <strong>persistent static geometry</strong> is preserved.
                This clean static representation enables <strong>accurate warping</strong> across viewpoints and
                maintains spatial coherence throughout the generated video.
            </p>

            <!-- 🔹 Part 2: Pipeline -->
            <h3>Dynamic Region Detection Pipeline</h3>
            <div class="image-container">
                <div class="image-content">
                <img src="images/fig_dynamicmask_pipeline.png" class="img large-image" alt="Dynamic Region Detection Pipeline">
                </div>
                <p class="image-caption">
                <strong>Three-stage dynamic masking pipeline.</strong> Motion detection → Tracking → Propagation.
                </p>
            </div>
            <p>
                Our <strong>Dynamic Masking Pipeline</strong> refines motion region detection through a
                three-stage process to produce complete object-level masks:
            </p>

            <p>
                (1) <strong>Dynamic Thresholding</strong> — Optical flow differences detect pixel-level motion.<br>
                (2) <strong>Backward Tracking</strong> — Using CoTracker3, sampled points are tracked backward
                across all frames to aggregate motion evidence and identify objects that move at any point.<br>
                (3) <strong>Mask Propagation</strong> — Aggregated motion cues in the first frame are propagated
                to the entire video via SAM2, generating clean masks that remove moving elements while retaining
                the static background for precise 3D reconstruction.
            </p>

            
            </section>

            <section id="results" class="section">
                <h2>Results</h2>
                <!-- <h3>Qualitative Comparisons</h3>
                <div class="slideshow-container">
                    <div class="slideshow" data-range="001-009" data-dir="videos" data-ext="mp4"></div>
                    <button class="slideshow-nav prev" aria-label="Previous slide" title="Previous">&#10094;</button>
                    <button class="slideshow-nav next" aria-label="Next slide" title="Next">&#10095;</button>
                </div> -->
                <!-- <h2>Results</h2> -->
                <h3>Evaluation of Spatial and Geometric Consistency</h3>
                <div class="table-container">
                <div class="table-wrap">
                    <table class="advantages-table">
                    <thead>
                        <tr>
                        <th rowspan="2" class="left">Methods</th>
                        <th colspan="4">RealEstate10K</th>
                        <th colspan="4">DynPose-100K</th>
                        </tr>
                        <tr>
                        <th>PSNR <span class="up">↑</span></th>
                        <th>SSIM <span class="up">↑</span></th>
                        <th>LPIPS <span class="down">↓</span></th>
                        <th>MEt3R <span class="down">↓</span></th>
                        <th>PSNR <span class="up">↑</span></th>
                        <th>SSIM <span class="up">↑</span></th>
                        <th>LPIPS <span class="down">↓</span></th>
                        <th>MEt3R <span class="down">↓</span></th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                        <td class="left">DFoT</td>
                        <td>18.30</td><td>0.596</td><td>0.308</td><td>0.181</td>
                        <td>12.15</td><td>0.304</td><td>0.417</td><td>0.183</td>
                        </tr>
                        <tr class="ours">
                        <td class="left first"><strong>3DScenePrompt (Ours)</strong></td>
                        <td class="first"><strong>20.89</strong></td><td class="first"><strong>0.717</strong></td>
                        <td class="first"><strong>0.212</strong></td><td class="first"><strong>0.0408</strong></td>
                        <td class="first"><strong>13.05</strong></td><td class="first"><strong>0.367</strong></td>
                        <td class="first"><strong>0.381</strong></td><td class="first"><strong>0.124</strong></td>
                        </tr>
                    </tbody>
                    </table>
                    <p class="image-caption" style="text-align: center; margin-top: 10px;">
                    <strong>Evaluation of spatial and geometric consistency.</strong>
                    We compare DFoT and <strong>3DScenePrompt</strong> on
                    RealEstate10K and DynPose-100K. PSNR, SSIM, and LPIPS evaluate spatial consistency,
                    while MEt3R measures geometric accuracy.
                    </p>
                </div>
                </div>
                <h3>Camera Controllability Evaluation</h3>
                <div class="table-container">
                <div class="table-wrap">
                    <table class="advantages-table">
                    <thead>
                        <tr>
                        <th rowspan="2" class="left">Methods</th>
                        <th colspan="3">DynPose-100K</th>
                        </tr>
                        <tr>
                        <th>mRotErr (°) <span class="down">↓</span></th>
                        <th>mTransErr <span class="down">↓</span></th>
                        <th>mCamMC <span class="down">↓</span></th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                        <td class="left">MotionCtrl</td>
                        <td>3.5654</td><td>7.8231</td><td>9.7834</td>
                        </tr>
                        <tr>
                        <td class="left">CameraCtrl</td>
                        <td>3.3273</td><td>9.5989</td><td>11.2122</td>
                        </tr>
                        <tr>
                        <td class="left">FloVD</td>
                        <td>3.4811</td><td>11.0302</td><td>12.6202</td>
                        </tr>
                        <tr>
                        <td class="left">AC3D</td>
                        <td>3.0675</td><td>9.7044</td><td>11.1634</td>
                        </tr>
                        <tr>
                        <td class="left">DFoT</td>
                        <td>2.3977</td><td>8.0866</td><td>9.2330</td>
                        </tr>
                        <tr class="ours">
                        <td class="left first"><strong>3DScenePrompt (Ours)</strong></td>
                        <td class="first"><strong>2.3772</strong></td>
                        <td class="first"><strong>7.4174</strong></td>
                        <td class="first"><strong>8.6352</strong></td>
                        </tr>
                    </tbody>
                    </table>
                    <p class="image-caption" style="text-align:center; margin-top:10px;">
                    <strong>Evaluation of camera controllability.</strong>
                    Lower mRotErr, mTransErr, and mCamMC indicate more accurate control over camera pose transitions.
                    </p>
                </div>
                </div>


                <h3>Video Generation Quality Evaluation</h3>
                <div class="table-container">
                <div class="table-wrap">
                    <table class="advantages-table">
                    <thead>
                        <tr>
                        <th rowspan="2" class="left">Methods</th>
                        <th colspan="9">DynPose-100K</th>
                        </tr>
                        <tr>
                        <th>FVD <span class="down">↓</span></th>
                        <th>Overall Score <span class="up">↑</span></th>
                        <th>Subject Consist <span class="up">↑</span></th>
                        <th>Bg Consist <span class="up">↑</span></th>
                        <th>Aesthetic Quality <span class="up">↑</span></th>
                        <th>Imaging Quality <span class="up">↑</span></th>
                        <th>Temporal Flicker <span class="up">↑</span></th>
                        <th>Motion Smooth <span class="up">↑</span></th>
                        <th>Dynamic Degree <span class="up">↑</span></th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                        <td class="left">MotionCtrl</td>
                        <td>1017.42</td><td>0.5625</td><td>0.5158</td><td>0.7093</td><td>0.3157</td><td>0.3149</td><td>0.8297</td><td>0.8432</td><td>0.7900</td>
                        </tr>
                        <tr>
                        <td class="left">CameraCtrl</td>
                        <td>737.05</td><td>0.6280</td><td>0.6775</td><td>0.8238</td><td>0.3736</td><td>0.3888</td><td>0.6837</td><td>0.6955</td><td>0.9900</td>
                        </tr>
                        <tr>
                        <td class="left">FloVD</td>
                        <td>171.27</td><td>0.7273</td><td>0.7964</td><td>0.8457</td><td>0.4722</td><td>0.5546</td><td>0.7842</td><td>0.8364</td><td>0.9900</td>
                        </tr>
                        <tr>
                        <td class="left">AC3D</td>
                        <td>281.21</td><td>0.7428</td><td>0.8360</td><td>0.8674</td><td>0.4766</td><td>0.5381</td><td>0.8020</td><td>0.8673</td><td>1.0000</td>
                        </tr>
                        <tr class="ours">
                        <td class="left first"><strong>3DScenePrompt (Ours)</strong></td>
                        <td class="first"><strong>127.48</strong></td>
                        <td class="first"><strong>0.7747</strong></td>
                        <td class="first"><strong>0.8669</strong></td>
                        <td class="first"><strong>0.8727</strong></td>
                        <td class="first"><strong>0.4990</strong></td>
                        <td class="first"><strong>0.5964</strong></td>
                        <td class="first"><strong>0.8551</strong></td>
                        <td class="first"><strong>0.9260</strong></td>
                        <td class="first"><strong>1.0000</strong></td>
                        </tr>
                    </tbody>
                    </table>
                    <p class="image-caption" style="text-align:center; margin-top:10px;">
                    <strong>Evaluation of video generation quality.</strong>
                    FVD evaluates overall temporal quality (lower is better).  
                    VBench++ metrics measure subject and background consistency, perceptual quality, and temporal smoothness (higher is better).
                    </p>
                </div>
                </div>

                <h3>Ablation Study</h3>
                <div class="table-container">
                <div class="table-wrap">
                    <table class="advantages-table">
                    <thead>
                        <tr>
                        <th rowspan="2" class="left">Methods</th>
                        <th rowspan="2">Dynamic Mask&nbsp;&nbsp;<span style="font-family:monospace;">𝓜</span></th>
                        <th colspan="4">DynPose-100K</th>
                        </tr>
                        <tr>
                        <th>PSNR <span class="up">↑</span></th>
                        <th>SSIM <span class="up">↑</span></th>
                        <th>LPIPS <span class="down">↓</span></th>
                        <th>MEt3R <span class="down">↓</span></th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                        <td class="left">Ours (n=1)</td>
                        <td>✓</td>
                        <td>13.0207</td><td>0.3732</td><td>0.3771</td><td>0.1248</td>
                        </tr>
                        <tr>
                        <td class="left">Ours (n=4)</td>
                        <td>✓</td>
                        <td>13.0382</td><td>0.3733</td><td>0.3758</td><td>0.1249</td>
                        </tr>
                        <tr>
                        <td class="left">Ours (n=L)</td>
                        <td>✓</td>
                        <td>13.0206</td><td>0.3631</td><td>0.3810</td><td>0.1235</td>
                        </tr>
                        <tr>
                        <td class="left">Ours (n=7)</td>
                        <td>✗</td>
                        <td>12.2304</td><td>0.3063</td><td>0.3821</td><td>0.1349</td>
                        </tr>
                        <tr class="ours">
                        <td class="left first"><strong>Ours (n=7)</strong></td>
                        <td class="first">✓</td>
                        <td class="first"><strong>13.0468</strong></td>
                        <td class="first"><strong>0.3666</strong></td>
                        <td class="first"><strong>0.3812</strong></td>
                        <td class="first"><strong>0.1242</strong></td>
                        </tr>
                    </tbody>
                    </table>

                    <p class="image-caption" style="text-align:center; margin-top:10px;">
                    <strong>Ablation study on dynamic masking.</strong>
                    Removing the dynamic mask (<span style="font-family:monospace;">𝓜</span>) leads to degraded PSNR, SSIM,
                    and higher MEt3R, confirming its importance for scene-consistent video generation. Here, <strong>n</strong> denotes the <em>number of retrieved frames</em> used for spatial conditioning.
                    </p>
                </div>
                </div>
            </section>
            <!-- Contents: Conclusion -->
            <!-- <section id="conclusion" class="section">
                <h2>Conclusion</h2>

                <p>In this work, we propose MATRIX, a simple yet effective regularization terms that align the attention maps of 3D full attention in the interaction-dominant layers with ground truth instance mask tracks of subject/object/verb regions. 
                    Our approach significantly improves interaction fidelity, strengthens noun/verb grounding, and reduces identity drift and duplication without degrading overall video quality. Ablations further highlight the critical role of layer selection and the complementary contributions of SGA and SPA.</p>
            </section> -->
            <!-- Contents: Citation -->
            <div class="citation add-top-padding">
                <h1 id="citation">Citation</h1>
                <p> If you use this work or find it helpful, please consider citing: </p>
                <pre id="codecell0">
@misc{
}
                </pre>
            </div>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p class="credit">Credit: The design of this project page is inspired by previous academic project pages, such as <a href="https://llm-grounded-diffusion.github.io/" target="_blank">LLM-grounded Diffusion</a>, <a href="https://describe-anything.github.io/" target="_blank">Describe-anything</a>, <a href="https://cvlab-kaist.github.io/VIRAL" target="_blank">VIRAL</a>, and <a href="https://cvlab-kaist.github.io/MATRIX/" target="_blank">MATRIX</a>.</p>
        </div>
    </footer>

    <script>
    function toggleMute(element) {
        const video = element.parentElement.querySelector('video');
        const icon = element.querySelector('i');
        const text = element.querySelector('.unmute-text');
        
        if (video.muted) {
            video.muted = false;
            icon.className = 'fa fa-volume-up';
            text.textContent = 'Mute';
        } else {
            video.muted = true;
            icon.className = 'fa fa-volume-off';
            text.textContent = 'Click to unmute';
        }
    }
    const captions = {
        "000": "At a kitchen counter, a student in a white shirt places a notebook on a wooden table. Cups and plates are scattered nearby, and sunlight comes in through the window. The action demonstrates placing the notebook on the flat surface.",
        "001": "A man in a black cap and t-shirt stands at the open door of a luxury car, conversing with another man inside the vehicle, in a parking lot with several cars and a bench under a clear sky, suggesting a casual yet professional setting. Two seconds later, the scene shifts to a sunlit parking lot where the same man in the black cap and t-shirt, now wearing a surgical mask, continues his conversation with another man in a black polo shirt and jeans, who is inside a black luxury sedan. The background features several cars and a building with a glass facade, indicating a commercial area.",
        "002": "A young man with a beard and tied-back hair is seen standing in a modern, compact kitchen, holding a spoon over a cup at the sink, surrounded by cooking utensils and ingredients, indicating he is preparing a meal. The kitchen features a stainless steel sink, a black frying pan, and a cutting board with cheese and an orange, under warm lighting. Two seconds later, the same man, now identified by his bun hairstyle, is focused on cooking in the efficient kitchen, which includes a stove, a frying pan, and a cutting board with cheese and an orange slice. The kitchen is equipped with a refrigerator and open shelving filled with dishes and bottles, under a white subway tile backsplash.",
        "003": "A bride in a white gown and a veil and a groom in a beige suit with long dreadlocks walk hand in hand down a narrow, unpaved path surrounded by lush greenery under an overcast sky, creating a serene atmosphere. The scene is intimate, with no other people or structures in sight. Two seconds later, the couple continues their walk on a secluded path, the bride's dress flowing and the groom's suit blending with the natural surroundings. The soft light from the overcast sky enhances the romantic and peaceful mood of their wedding day.",
        "004": "A man in sunglasses and a black shirt stands in a narrow alleyway, looking directly at the camera with a neutral expression. The alley is flanked by ancient stone walls, and the scene is bathed in sunlight, casting sharp shadows and highlighting the textures of the stones. The clear blue sky suggests a warm, sunny day in a historic urban setting. As time passes, the man remains in the alley, the sunlight continuing to accentuate the textures of the stone walls and the cobblestone path, maintaining the tranquil atmosphere of the scene.",
        "005": "Four men are gathered in a cozy living room, engaging in a lively discussion. Initially, they are seen seated on a beige sofa with a red and white striped cushion, a dark wooden armchair, and a glass-topped coffee table, surrounded by a framed picture, a mirror, and a plant, with a 'Coca-Cola' sign in the background. Later, the setting shifts slightly with two men now on a sofa, one holding a glass bottle, and the other barefoot, while two others sit on a gray armchair and a stool, respectively, with an 'Arsenal' sign visible, indicating a sports-related theme.",
        "006": "A shirtless man with a well-defined physique is seen performing push-ups on artificial turf in a gym, wearing a white cap and black shorts. He is surrounded by various fitness equipment, including stationary bikes and a rowing machine, under bright lighting. His expression is one of intense focus, highlighting the seriousness of his workout. The same scene continues with the man maintaining his push-up position, emphasizing his muscular physique and the intensity of his exercise routine. Contrary to the detailed descriptions, the overall summary mistakenly mentions the man wearing a red shirt, which is not supported by the detailed frame descriptions.",
        "007": "The video features a bustling city street scene with pedestrians and vehicles, including a red bus and a white van, moving along the road. The urban landscape is dotted with green trees and modern buildings, with a clear blue sky overhead. A red planter box with a blue broom and yellow bucket is visible on the sidewalk, adding a splash of color. As the video continues, the scene shifts to a sunny day in Shanghai, where people walk and converse on the sidewalks. The architecture combines traditional and modern styles, with a leafless tree and a red bench with greenery adding to the urban charm. Overlaying text in Chinese suggests the footage is from a Chinese media source.",
        "008": "A young man with slicked-back hair and a dark blue t-shirt is seen in a food court, holding a partially eaten hot dog and looking at it with a focused expression. The background is blurred, indicating a bustling environment with other patrons and food items. Two seconds later, the same man, now with slicked-back dark hair and a black backpack, takes a selfie while holding a sesame seed-encrusted baguette. His content expression and the lively atmosphere of the food court are highlighted by the blurred background. Contrary to the detailed scenes, the overall description inaccurately mentions a young woman eating a hot dog.",
        "009": "A woman with blonde hair, dressed in a red top and floral skirt, is seen standing in a modern showroom, gesturing towards a black sports car with red accents. The showroom features a minimalist design with a glass partition, a sleek black chair, and a table with a laptop, all under bright lighting that highlights the car's glossy finish and the woman's attire. Two seconds later, the same woman, now in a red sweater and floral skirt, stands at the threshold of a glass meeting room, observing a man seated at a table with a laptop, engaged in a phone conversation. The room's minimalist design, with a whiteboard and black chairs, suggests a professional setting.",
        "010": "The man in a blue shirt feeds a strawberry to the woman in a white chef coat.",
    };

    document.querySelectorAll('.slideshow[data-range]').forEach(slideshow => {
    const [startStr, endStr] = slideshow.dataset.range.split('-').map(s => s.trim());
    const pad = Math.max(startStr.length, endStr.length);
    const start = parseInt(startStr, 10), end = parseInt(endStr, 10);
    const dir = (slideshow.dataset.dir || '').replace(/\/?$/, '/');
    const ext = (slideshow.dataset.ext || 'mp4').replace(/^\./, '');

    for (let i = start; i <= end; i++) {
        const id = String(i).padStart(pad, '0');

        const fig = document.createElement('figure');
        fig.className = 'slide';
        if (i === start) fig.classList.add('active');

        const video = document.createElement('video');
        video.className = 'video';
        video.autoplay = true; video.muted = true; video.loop = true; video.playsInline = true;

        const source = document.createElement('source');
        source.src = `${dir}${id}.${ext}`;
        source.type = `video/${ext}`;
        video.appendChild(source);

        const cap = document.createElement('figcaption');
        cap.className = 'caption';
        cap.textContent = captions[id] || `[${id}]`;

        fig.appendChild(video);
        fig.appendChild(cap);
        slideshow.appendChild(fig);
    }
    });
    document.addEventListener('DOMContentLoaded', function() {
        const videos = document.querySelectorAll('video');
        videos.forEach(video => {
            video.addEventListener('play', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
            
            video.addEventListener('pause', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
        });

    // Initialize all slideshows
    document.querySelectorAll('.slideshow-container').forEach(container => {
        const slideshow = container.querySelector('.slideshow');
        if (!slideshow) return;
        const slides = slideshow.querySelectorAll('.slide');
        if (!slides.length) return;
        const prevButton = container.querySelector('.slideshow-nav.prev');
        const nextButton = container.querySelector('.slideshow-nav.next');
        const playPauseButton = container.querySelector('.play-pause');
        
        let currentSlide = 0;
        let autoplayInterval = null;
        let isPlaying = true;


        const dotsContainer = container.querySelector('.slideshow-dots');
            if (dotsContainer) {
                dotsContainer.innerHTML = ''; // 초기화

                // dot 생성
                slides.forEach((_, i) => {
                    const dot = document.createElement('div');
                    dot.className = 'slideshow-dot';
                    if (i === 0) dot.classList.add('active');
                    dot.addEventListener('click', () => showSlide(i));
                    dotsContainer.appendChild(dot);
                });
            }

            // dot 상태 갱신 함수
            function updateDots() {
                if (!dotsContainer) return;
                const dots = dotsContainer.querySelectorAll('.slideshow-dot');
                dots.forEach((dot, i) => {
                    dot.classList.toggle('active', i === currentSlide);
                });
            }

        function showSlide(n) {
            slides.forEach(slide => slide.classList.remove('active'));
            currentSlide = (n + slides.length) % slides.length;
            slides[currentSlide].classList.add('active');

            slides.forEach(s => { const v = s.querySelector('video'); if (v) v.pause(); });
            const v = slides[currentSlide].querySelector('video');
            if (v) { v.muted = true; v.play().catch(()=>{}); }
            updateDots();
        }

        function startAutoplay() {
            if (autoplayInterval || slides.length < 2) return;
            autoplayInterval = setInterval(() => showSlide(currentSlide + 1), 5000);
            isPlaying = true;
            if (playPauseButton) playPauseButton.innerHTML = '<i class="fa fa-pause"></i>';
        }

        function stopAutoplay() {
            if (autoplayInterval) clearInterval(autoplayInterval);
            autoplayInterval = null;
            isPlaying = false;
            if (playPauseButton) playPauseButton.innerHTML = '<i class="fa fa-play"></i>';
        }

        function changeSlide(step) {
            showSlide(currentSlide + step);

            // 🔹 새 슬라이드의 비디오를 즉시 재생
            const activeSlide = slides[currentSlide];
            const video = activeSlide.querySelector('video');
            if (video) {
                video.currentTime = 0; // 처음부터 재생
                video.play().catch(() => {}); // 자동 재생 실패 방지
            }

            if (isPlaying) { 
                stopAutoplay(); 
                startAutoplay(); 
            }
        }


        // Initialize this slideshow
        showSlide(0);
        stopAutoplay();
        // startAutoplay();

        // Add event listeners
        if (prevButton) prevButton.addEventListener('click', () => changeSlide(-1));
        if (nextButton) nextButton.addEventListener('click', () => changeSlide(1));
        if (playPauseButton) playPauseButton.addEventListener('click', () => {
            isPlaying ? stopAutoplay() : startAutoplay();
        });
    });

    

    // Handle main video play button
    const mainVideo = document.querySelector('.main-video');
    const playButton = document.querySelector('.play-button-overlay');
    
    if (mainVideo && playButton) {
        // Click play button to play video
        playButton.addEventListener('click', () => {
            mainVideo.play();
            mainVideo.classList.add('playing');
        });

        // Handle video play/pause events
        mainVideo.addEventListener('play', () => {
            mainVideo.classList.add('playing');
        });

        mainVideo.addEventListener('pause', () => {
            mainVideo.classList.remove('playing');
        });

        mainVideo.addEventListener('ended', () => {
            mainVideo.classList.remove('playing');
        });
    }

    // -----------------------------
    // 🖼️ Click-to-zoom Lightbox
    // -----------------------------
    // Build overlay once
    const lightbox = document.createElement('div');
    lightbox.className = 'lightbox-overlay';
    lightbox.setAttribute('role', 'dialog');
    lightbox.setAttribute('aria-modal', 'true');
    lightbox.innerHTML = '<img alt="Expanded image">';
    document.body.appendChild(lightbox);
    const lightboxImg = lightbox.querySelector('img');

    function openLightbox(src, alt) {
        lightboxImg.src = src;
        lightboxImg.alt = alt || '';
        lightbox.classList.add('active');
        document.body.classList.add('no-scroll');
    }
    function closeLightbox() {
        lightbox.classList.remove('active');
        document.body.classList.remove('no-scroll');
        lightboxImg.src = '';
    }

    // Close on click anywhere or on Esc
    lightbox.addEventListener('click', closeLightbox);
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && lightbox.classList.contains('active')) closeLightbox();
    });

    // Mark target images as zoomable and wire up click
    const zoomableImages = document.querySelectorAll('.image-container img, .slideshow img, .main-content img.img, .hero-section img.img, .slideshow video');
    zoomableImages.forEach(img => {
        img.classList.add('zoomable');
        img.addEventListener('click', () => {
            // support optional high-res source via data-fullsrc
            const src = img.getAttribute('data-fullsrc') || img.currentSrc || img.src;
            openLightbox(src, img.alt);
        });
    });
});
    </script>
</body>
</html>
